{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-08-24T16:58:16.205419Z",
          "iopub.status.busy": "2025-08-24T16:58:16.205197Z",
          "iopub.status.idle": "2025-08-24T17:00:10.954410Z",
          "shell.execute_reply": "2025-08-24T17:00:10.953602Z",
          "shell.execute_reply.started": "2025-08-24T16:58:16.205400Z"
        },
        "id": "t2Lwn1cCb7fq",
        "outputId": "0328a47c-746d-4c7d-b5c7-e1b6e9ec702e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scikit-learn==1.3.0\n",
            "  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.4.3\n",
            "  Downloading matplotlib-3.4.3.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting seaborn==0.11.2\n",
            "  Downloading seaborn-0.11.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting xgboost==1.6.1\n",
            "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (3.6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.4.3) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.4.3) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.4.3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.4.3) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n",
            "Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.9/192.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-3.4.3-cp311-cp311-linux_x86_64.whl size=10441363 sha256=2db785b9539ca944920dfcea9aa7aedc6fff1c3c6150b14450db9f5dd9defc10\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/99/fa/d6b0fc4a7c0f444b5f0ee8e7513f1d3095338bc1b458033e5d\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: numpy, pandas, matplotlib, xgboost, seaborn, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.2\n",
            "    Uninstalling matplotlib-3.7.2:\n",
            "      Successfully uninstalled matplotlib-3.7.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.0.3\n",
            "    Uninstalling xgboost-2.0.3:\n",
            "      Successfully uninstalled xgboost-2.0.3\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.12.2\n",
            "    Uninstalling seaborn-0.12.2:\n",
            "      Successfully uninstalled seaborn-0.12.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
            "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\n",
            "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\n",
            "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\n",
            "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
            "ydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.4.3 which is incompatible.\n",
            "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "cartopy 0.24.1 requires matplotlib>=3.6, but you have matplotlib 3.4.3 which is incompatible.\n",
            "ipympl 0.9.7 requires matplotlib<4,>=3.5.0, but you have matplotlib 3.4.3 which is incompatible.\n",
            "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.25.2 which is incompatible.\n",
            "mne 1.9.0 requires matplotlib>=3.6, but you have matplotlib 3.4.3 which is incompatible.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
            "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
            "arviz 0.21.0 requires matplotlib>=3.5, but you have matplotlib 3.4.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "blosc2 3.5.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.4.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
            "bigframes 2.8.0 requires matplotlib>=3.7.1, but you have matplotlib 3.4.3 which is incompatible.\n",
            "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.4.3 numpy-1.25.2 pandas-2.0.3 scikit-learn-1.3.0 seaborn-0.11.2 xgboost-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scikit-learn==1.3.0 matplotlib==3.4.3 seaborn==0.11.2 xgboost==1.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGB 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-24T17:00:10.956882Z",
          "iopub.status.busy": "2025-08-24T17:00:10.956616Z",
          "iopub.status.idle": "2025-08-24T19:32:32.502733Z",
          "shell.execute_reply": "2025-08-24T19:32:32.502055Z",
          "shell.execute_reply.started": "2025-08-24T17:00:10.956859Z"
        },
        "id": "Q_mRsq-_b7fq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import sklearn\n",
        "import xgboost\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "import random as rn\n",
        "RANDOM_SEED = 2025\n",
        "np.random.seed(RANDOM_SEED)\n",
        "rn.seed(RANDOM_SEED)\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def smape(gt, preds):\n",
        "    gt= np.array(gt)\n",
        "    preds = np.array(preds)\n",
        "    v = 2 * abs(preds - gt) / (abs(preds) + abs(gt))\n",
        "    score = np.mean(v) * 100\n",
        "    return score\n",
        "\n",
        "def weighted_mse(alpha = 1):\n",
        "    def weighted_mse_fixed(label, pred):\n",
        "        residual = (label - pred).astype(\"float\")\n",
        "        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n",
        "        hess = np.where(residual>0, 2*alpha, 2.0)\n",
        "        return grad, hess\n",
        "    return weighted_mse_fixed\n",
        "\n",
        "def custom_smape(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    return 'custom_smape', np.mean(2 * abs(preds - labels) / (abs(preds) + abs(labels))) * 100\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/dataset-ee/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/dataset-ee/test.csv')\n",
        "building_info = pd.read_csv('/kaggle/input/dataset-ee/building_info.csv')\n",
        "\n",
        "train = train.rename(columns={\n",
        "    '건물번호': 'building_number',\n",
        "    '일시': 'date_time',\n",
        "    '기온(°C)': 'temperature',\n",
        "    '강수량(mm)': 'rainfall',\n",
        "    '풍속(m/s)': 'windspeed',\n",
        "    '습도(%)': 'humidity',\n",
        "    '일조(hr)': 'sunshine',\n",
        "    '일사(MJ/m2)': 'solar_radiation',\n",
        "    '전력소비량(kWh)': 'power_consumption'\n",
        "})\n",
        "train.drop('num_date_time', axis = 1, inplace=True)\n",
        "\n",
        "test = test.rename(columns={\n",
        "    '건물번호': 'building_number',\n",
        "    '일시': 'date_time',\n",
        "    '기온(°C)': 'temperature',\n",
        "    '강수량(mm)': 'rainfall',\n",
        "    '풍속(m/s)': 'windspeed',\n",
        "    '습도(%)': 'humidity',\n",
        "    '일조(hr)': 'sunshine',\n",
        "    '일사(MJ/m2)': 'solar_radiation',\n",
        "    '전력소비량(kWh)': 'power_consumption'\n",
        "})\n",
        "test.drop('num_date_time', axis = 1, inplace=True)\n",
        "\n",
        "building_info = building_info.rename(columns={\n",
        "    '건물번호': 'building_number',\n",
        "    '건물유형': 'building_type',\n",
        "    '연면적(m2)': 'total_area',\n",
        "    '냉방면적(m2)': 'cooling_area',\n",
        "    '태양광용량(kW)': 'solar_power_capacity',\n",
        "    'ESS저장용량(kWh)': 'ess_capacity',\n",
        "    'PCS용량(kW)': 'pcs_capacity'\n",
        "})\n",
        "\n",
        "translation_dict = {\n",
        "    '건물기타': 'Other Buildings',\n",
        "    '공공': 'Public',\n",
        "    '학교': 'University',\n",
        "    '백화점': 'Department Store',\n",
        "    '병원': 'Hospital',\n",
        "    '상용': 'Commercial',\n",
        "    '아파트': 'Apartment',\n",
        "    '연구소': 'Research Institute',\n",
        "    'IDC(전화국)': 'IDC',\n",
        "    '호텔': 'Hotel'\n",
        "}\n",
        "\n",
        "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
        "\n",
        "building_info['solar_power_utility'] = np.where(building_info.solar_power_capacity !='-',1,0)\n",
        "building_info['ess_utility'] = np.where(building_info.ess_capacity !='-',1,0)\n",
        "\n",
        "train = pd.merge(train, building_info, on='building_number', how='left')\n",
        "test = pd.merge(test, building_info, on='building_number', how='left')\n",
        "\n",
        "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
        "test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
        "\n",
        "# date time feature 생성\n",
        "train['hour'] = train['date_time'].dt.hour\n",
        "train['day'] = train['date_time'].dt.day\n",
        "train['month'] = train['date_time'].dt.month\n",
        "train['day_of_week'] = train['date_time'].dt.dayofweek #요일(월=0, 일=6)\n",
        "# train['week'] = train.dt.isocalendar().week.astype(np.int32)\n",
        "test['hour'] = test['date_time'].dt.hour\n",
        "test['day'] = test['date_time'].dt.day\n",
        "test['month'] = test['date_time'].dt.month\n",
        "test['day_of_week'] = test['date_time'].dt.dayofweek #요일\n",
        "# test['week'] = test.dt.isocalendar().week.astype(np.int32)\n",
        "\n",
        "def calculate_day_values(dataframe, target_column, output_column, aggregation_func):\n",
        "    result_dict = {}\n",
        "    grouped_temp = dataframe.groupby(['building_number', 'month', 'day'])[target_column].agg(aggregation_func)\n",
        "    for (building, month, day), value in grouped_temp.items():\n",
        "        result_dict.setdefault(building, {}).setdefault(month, {})[day] = value\n",
        "    dataframe[output_column] = [\n",
        "        result_dict.get(row['building_number'], {}).get(row['month'], {}).get(row['day'], None)\n",
        "        for _, row in dataframe.iterrows()\n",
        "    ]\n",
        "\n",
        "train['day_max_temperature'] = 0.0\n",
        "train['day_mean_temperature'] = 0.0\n",
        "calculate_day_values(train, 'temperature', 'day_max_temperature', 'max')\n",
        "calculate_day_values(train, 'temperature', 'day_mean_temperature', 'mean')\n",
        "calculate_day_values(train, 'temperature', 'day_min_temperature', 'min')\n",
        "train['day_temperature_range'] = train['day_max_temperature'] - train['day_min_temperature']\n",
        "calculate_day_values(test, 'temperature', 'day_max_temperature', 'max')\n",
        "calculate_day_values(test, 'temperature', 'day_mean_temperature', 'mean')\n",
        "calculate_day_values(test, 'temperature', 'day_min_temperature', 'min')\n",
        "test['day_temperature_range'] = test['day_max_temperature'] - test['day_min_temperature']\n",
        "\n",
        "######################## 특이값 제거 ########################\n",
        "out_ind_li = []\n",
        "# 1번\n",
        "out_ind_1 = train.loc[(train['building_number'] ==1 ) & (train['power_consumption'] < 3000) ,:].index\n",
        "out_ind_li += list(out_ind_1)\n",
        "# 5번 (날짜 조건)\n",
        "out_ind_5 = train.loc[\n",
        "    (train['building_number'] == 5) &\n",
        "    (train['date_time'].dt.strftime('%Y%m%d').astype(int) >= 20240801) &\n",
        "    (train['date_time'].dt.strftime('%Y%m%d').astype(int) <= 20240815) &\n",
        "    (train['power_consumption'] < 3200)\n",
        ", :].index\n",
        "out_ind_li += list(out_ind_5)\n",
        "# 7번\n",
        "out_ind_7 = train.loc[(train['building_number'] ==7 ) & (train['power_consumption'] < 3000) ,:].index\n",
        "out_ind_li += list(out_ind_7)\n",
        "# 12번\n",
        "out_ind_12 = train.loc[(train['building_number'] ==12 ) & (train['power_consumption'] < 4000),:].index\n",
        "out_ind_li += list(out_ind_12)\n",
        "# 17번 (날짜 조건)\n",
        "out_ind_17 = train.loc[\n",
        "    (train['building_number'] == 17) &\n",
        "    (train['date_time'].dt.strftime('%Y%m%d').astype(int) >= 20240619) &\n",
        "    (train['date_time'].dt.strftime('%Y%m%d').astype(int) <= 20240701) &\n",
        "    (train['power_consumption'] < 1500)\n",
        ", :].index\n",
        "out_ind_li += list(out_ind_17)\n",
        "# 19번\n",
        "out_ind_19 = train.loc[(train['building_number'] ==19 ) & (train['power_consumption'] > 3000) ,:].index\n",
        "out_ind_li += list(out_ind_19)\n",
        "# 26번\n",
        "out_ind_26 = train.loc[(train['building_number'] ==26 ) & (train['power_consumption'] < 500) ,:].index\n",
        "out_ind_li += list(out_ind_26)\n",
        "# 30번\n",
        "out_ind_30 = train.loc[(train['building_number'] ==30 ) & (train['power_consumption'] < 9000) ,:].index\n",
        "out_ind_li += list(out_ind_30)\n",
        "# 41번\n",
        "out_ind_41 = train.loc[(train['building_number'] ==41 ) & (train['power_consumption'] < 2500),:].index\n",
        "out_ind_li += list(out_ind_41)\n",
        "# 42번\n",
        "out_ind_42 = train.loc[(train['building_number'] ==42 ) & (train['power_consumption'] < 3000) ,:].index\n",
        "out_ind_li+=list(out_ind_42)\n",
        "\n",
        "# 44번 보류\n",
        "out_ind_44 = train.loc[(train['building_number'] ==44 ) & (train['power_consumption'] < 900) ,:].index\n",
        "out_ind_li+=list(out_ind_44)\n",
        "# 52번 보류\n",
        "out_ind_52 = train.loc[(train['building_number'] ==52 ) & (train['power_consumption'] < 4500) ,:].index\n",
        "out_ind_li+=list(out_ind_52)\n",
        "\n",
        "# 67번\n",
        "out_ind_67 = train.loc[(train['building_number'] ==67 ) & (train['power_consumption'] < 8000) ,:].index\n",
        "out_ind_li+=list(out_ind_67)\n",
        "# 68번 보류\n",
        "out_ind_68 = train.loc[(train['building_number'] ==68 ) & (train['power_consumption'] < 1000) ,:].index\n",
        "out_ind_li+=list(out_ind_68)\n",
        "\n",
        "# 72번\n",
        "out_ind_72 = train.loc[\n",
        "    (train['building_number'] == 72) &\n",
        "    (train['date_time'].dt.strftime('%Y%m%d').astype(int) >= 20240601) &\n",
        "    (train['date_time'].dt.strftime('%Y%m%d').astype(int) <= 20240615) &\n",
        "    (train['power_consumption'] > 1830)\n",
        ", :].index\n",
        "out_ind_li += list(out_ind_72)\n",
        "\n",
        "# 79번\n",
        "out_ind_79 = train.loc[(train['building_number'] ==79 ) & (train['power_consumption'] < 1500) ,:].index\n",
        "out_ind_li+=list(out_ind_79)\n",
        "# 92번\n",
        "out_ind_92 = train.loc[(train['building_number'] ==92 ) & (train['power_consumption'] < 200) ,:].index\n",
        "out_ind_li+=list(out_ind_92)\n",
        "\n",
        "# 실제 제거\n",
        "print(\"특이값 제거 index 개수:\", len(out_ind_li))\n",
        "train = train.drop(index=out_ind_li).reset_index(drop=True)\n",
        "\n",
        "outlier_idx = train.index[train['power_consumption'] == 0].tolist()\n",
        "print(\"제로 power_consumption 제거할 행 개수:\", len(outlier_idx))\n",
        "print(\"인덱스 예시:\", outlier_idx[:10])\n",
        "train.drop(index=outlier_idx, inplace=True)\n",
        "print(\"남은 행 개수:\", train.shape[0])\n",
        "\n",
        "# 휴일 지정 (IDC, 백화점 건물 제외) - 기본 규칙\n",
        "holi_weekday = ['2024-06-06', '2024-08-15']\n",
        "\n",
        "train['holiday'] = np.where(\n",
        "    (~train['building_type'].isin(['IDC', 'Department Store'])) &\n",
        "    ((train.day_of_week >= 5) | (train.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday))),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "test['holiday'] = np.where(\n",
        "    (~test['building_type'].isin(['IDC', 'Department Store'])) &\n",
        "    ((test.day_of_week >= 5) | (test.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday))),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "# >>> 추가 휴일 지정: 건물 27, 40, 59, 63에 대해 \"매달 둘째 주 일요일\"을 휴일로 처리\n",
        "# - pandas의 day_of_week: 월=0, ..., 일=6\n",
        "# - 둘째 주 일요일은 그 달의 날짜가 8~14 범위인 '일요일'\n",
        "target_buildings = {27, 40, 59, 63}\n",
        "# 27, 40, 59, 63\n",
        "def mark_second_and_fourth_sunday_holiday(df):\n",
        "    is_sunday = df['day_of_week'] == 6\n",
        "    # 둘째 주 (8~14일), 넷째 주 (22~28일)\n",
        "    is_2nd_week = df['day'].between(8, 14)\n",
        "    is_4th_week = df['day'].between(22, 28)\n",
        "    mask = df['building_number'].isin(target_buildings) & is_sunday & (is_2nd_week | is_4th_week)\n",
        "    df.loc[mask, 'holiday'] = 1\n",
        "    return df\n",
        "\n",
        "train = mark_second_and_fourth_sunday_holiday(train)\n",
        "test  = mark_second_and_fourth_sunday_holiday(test)\n",
        "# >>> 32번 건물: 매달 둘째·넷째 월요일만 휴무. (기존 holiday 규칙 모두 덮어씀)\n",
        "def apply_2nd_4th_monday_off_for_b32(df):\n",
        "    is_b32 = df['building_number'] == 32\n",
        "\n",
        "    # 우선 32번 건물은 전부 근무(0)로 초기화\n",
        "    df.loc[is_b32, 'holiday'] = 0\n",
        "\n",
        "    # 요일: 월=0, ..., 일=6\n",
        "    is_monday = df['day_of_week'] == 0\n",
        "\n",
        "    # 주차(week-of-month): 1~5\n",
        "    wom = ((df['day'] - 1) // 7) + 1\n",
        "\n",
        "    # 둘째/넷째 주 + 월요일인 곳만 휴무로 표시\n",
        "    mask = is_b32 & is_monday & wom.isin([2, 4])\n",
        "    df.loc[mask, 'holiday'] = 1\n",
        "    return df\n",
        "\n",
        "train = apply_2nd_4th_monday_off_for_b32(train)\n",
        "test  = apply_2nd_4th_monday_off_for_b32(test)\n",
        "# <<<\n",
        "# >>> 2번 건물: 매주 토요일만 휴무(공휴일 근무). 기존 규칙을 모두 덮어씀.\n",
        "def apply_sat_only_off(df):\n",
        "    is_b2 = df['building_number'] == 2\n",
        "    # 기본값: 전부 근무(holiday=0)\n",
        "    df.loc[is_b2, 'holiday'] = 0\n",
        "    # 토요일(월=0, ..., 토=5, 일=6)만 휴무\n",
        "    df.loc[is_b2 & (df['day_of_week'] == 5), 'holiday'] = 1\n",
        "    return df\n",
        "\n",
        "train = apply_sat_only_off(train)\n",
        "test  = apply_sat_only_off(test)\n",
        "\n",
        "no_holiday_buildings = [58, 82, 97]\n",
        "\n",
        "def clear_holiday_for_specific_buildings(df):\n",
        "    mask = df['building_number'].isin(no_holiday_buildings)\n",
        "    df.loc[mask, 'holiday'] = 0\n",
        "    return df\n",
        "\n",
        "train = clear_holiday_for_specific_buildings(train)\n",
        "test  = clear_holiday_for_specific_buildings(test)\n",
        "#시간\n",
        "train['sin_hour'] = np.sin(2 * np.pi * train['hour']/23.0)\n",
        "train['cos_hour'] = np.cos(2 * np.pi * train['hour']/23.0)\n",
        "test['sin_hour'] = np.sin(2 * np.pi * test['hour']/23.0)\n",
        "test['cos_hour'] = np.cos(2 * np.pi * test['hour']/23.0)\n",
        "\n",
        "#날짜\n",
        "train['sin_date'] = -np.sin(2 * np.pi * (train['month']+train['day']/31)/12)\n",
        "train['cos_date'] = -np.cos(2 * np.pi * (train['month']+train['day']/31)/12)\n",
        "test['sin_date'] = -np.sin(2 * np.pi * (test['month']+test['day']/31)/12)\n",
        "test['cos_date'] = -np.cos(2 * np.pi * (test['month']+test['day']/31)/12)\n",
        "\n",
        "#월\n",
        "train['sin_month'] = -np.sin(2 * np.pi * train['month']/12.0)\n",
        "train['cos_month'] = -np.cos(2 * np.pi * train['month']/12.0)\n",
        "test['sin_month'] = -np.sin(2 * np.pi * test['month']/12.0)\n",
        "test['cos_month'] = -np.cos(2 * np.pi * test['month']/12.0)\n",
        "\n",
        "#요일\n",
        "train['sin_dayofweek'] = -np.sin(2 * np.pi * (train['day_of_week']+1)/7.0)\n",
        "train['cos_dayofweek'] = -np.cos(2 * np.pi * (train['day_of_week']+1)/7.0)\n",
        "test['sin_dayofweek'] = -np.sin(2 * np.pi * (test['day_of_week']+1)/7.0)\n",
        "test['cos_dayofweek'] = -np.cos(2 * np.pi * (test['day_of_week']+1)/7.0)\n",
        "\n",
        "def CDH(xs):\n",
        "    cumsum = np.cumsum(xs - 26)\n",
        "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
        "\n",
        "def calculate_and_add_cdh(dataframe):\n",
        "    cdhs = []\n",
        "    for i in range(1, 101):\n",
        "        temp = dataframe[dataframe['building_number'] == i]['temperature'].values\n",
        "        cdh = CDH(temp)\n",
        "        cdhs.append(cdh)\n",
        "    return np.concatenate(cdhs)\n",
        "\n",
        "train['CDH'] = calculate_and_add_cdh(train)\n",
        "test['CDH'] = calculate_and_add_cdh(test)\n",
        "train['THI'] = 9/5*train['temperature'] - 0.55*(1-train['humidity']/100)*(9/5*train['humidity']-26)+32\n",
        "test['THI'] = 9/5*test['temperature'] - 0.55*(1-test['humidity']/100)*(9/5*test['humidity']-26)+32\n",
        "train['WCT'] = 13.12 + 0.6125*train['temperature'] - 11.37*(train['windspeed']**\n",
        "                                                            0.16) + 0.3965*(train['windspeed']**0.16)*train['temperature']\n",
        "test['WCT'] = 13.12 + 0.6125*test['temperature'] - 11.37*(test['windspeed']**\n",
        "                                                            0.16) + 0.3965*(test['windspeed']**0.16)*test['temperature']\n",
        "\n",
        "power_mean = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour', 'day_of_week'], aggfunc=np.mean).reset_index()\n",
        "power_mean.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_mean']\n",
        "\n",
        "power_std = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour', 'day_of_week'], aggfunc=np.std).reset_index()\n",
        "power_std.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_std']\n",
        "\n",
        "power_hour_mean = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour'], aggfunc=np.mean).reset_index()\n",
        "power_hour_mean.columns = ['building_number', 'hour', 'hour_mean']\n",
        "\n",
        "power_hour_std = pd.pivot_table(train, values='power_consumption', index=['building_number', 'hour'], aggfunc=np.std).reset_index()\n",
        "power_hour_std.columns = ['building_number', 'hour', 'hour_std']\n",
        "\n",
        "# Merge calculated features to 'train' and 'test' dataframes\n",
        "train = train.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "test = test.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "\n",
        "train = train.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "test = test.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "\n",
        "train = train.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
        "test = test.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
        "\n",
        "train = train.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
        "test = test.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "\n",
        "X = train.drop(['solar_power_capacity', 'ess_capacity', 'pcs_capacity',\n",
        "                'power_consumption','rainfall', 'sunshine', 'solar_radiation',\n",
        "                'hour','day','month','day_of_week','date_time'],axis =1 )\n",
        "\n",
        "Y = train[['building_type','power_consumption']]\n",
        "\n",
        "test_X = test.drop(['solar_power_capacity', 'ess_capacity', 'pcs_capacity','rainfall',\n",
        "                   'hour','month','day_of_week','day','date_time'], axis=1)\n",
        "\n",
        "type_list = []\n",
        "for value in train.building_type.values:\n",
        "    if value not in type_list:\n",
        "        type_list.append(value)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "KFOLD_SPLITS = 7\n",
        "\n",
        "def make_xgb():\n",
        "    return XGBRegressor(\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=5000,\n",
        "        max_depth=10,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.5,\n",
        "        min_child_weight=3,\n",
        "        random_state=RANDOM_SEED,\n",
        "        objective=weighted_mse(3),\n",
        "        tree_method=\"gpu_hist\",   # GPU 학습\n",
        "        gpu_id=0,                 # GPU 번호 (Colab, Kaggle은 0)\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "def cv_fit_predict(model_fn, X_tr, y_tr, X_te):\n",
        "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    oof = np.zeros(len(y_tr), dtype=float)\n",
        "    te_preds = []\n",
        "    Xv = X_tr.values\n",
        "    yv = y_tr.values\n",
        "    for tr_idx, va_idx in kf.split(Xv):\n",
        "        mdl = model_fn()\n",
        "        X_tr_f, X_va_f = Xv[tr_idx], Xv[va_idx]\n",
        "        y_tr_f, y_va_f = yv[tr_idx], yv[va_idx]\n",
        "        mdl.fit(X_tr_f, np.log(y_tr_f), eval_set=[(X_va_f, np.log(y_va_f))], verbose=False)\n",
        "        oof[va_idx] = np.exp(mdl.predict(X_va_f))\n",
        "        te_preds.append(np.exp(mdl.predict(X_te.values)))\n",
        "    te_mean = np.mean(te_preds, axis=0)\n",
        "    return oof, te_mean\n",
        "\n",
        "# === 1) GLOBAL 모델 ===\n",
        "# X, Y, test_X 는 기존 코드에서 만든 것과 동일 (building_type, building_number 포함)\n",
        "X_global  = X.drop(columns=[\"building_type\"])  # building_type 더미 안씀(전역)\n",
        "Xt_global = test_X.drop(columns=[\"building_type\"])\n",
        "oof_global, te_global = cv_fit_predict(make_xgb, X_global, Y[\"power_consumption\"], Xt_global)\n",
        "\n",
        "# === 2) TYPE 모델 ===\n",
        "oof_type  = np.zeros(len(Y), dtype=float)\n",
        "te_type   = np.zeros(len(test_X), dtype=float)\n",
        "for t in X[\"building_type\"].unique():\n",
        "    idx_tr = (X[\"building_type\"] == t)\n",
        "    idx_te = (test_X[\"building_type\"] == t)\n",
        "    x_t  = X.loc[idx_tr].copy()\n",
        "    xt_t = test_X.loc[idx_te].copy()\n",
        "    # building_number 원핫 (유형 내부 features 정렬)\n",
        "    x_t  = pd.get_dummies(x_t,  columns=[\"building_number\"], drop_first=False)\n",
        "    xt_t = pd.get_dummies(xt_t, columns=[\"building_number\"], drop_first=False)\n",
        "    xt_t = xt_t.reindex(columns=x_t.columns, fill_value=0)\n",
        "\n",
        "    # drop type col\n",
        "    x_t = x_t.drop(columns=[\"building_type\"])\n",
        "    xt_keep = xt_t.drop(columns=[\"building_type\"])\n",
        "\n",
        "    oof_t, te_t = cv_fit_predict(make_xgb, x_t, Y.loc[idx_tr, \"power_consumption\"], xt_keep)\n",
        "    oof_type[idx_tr.values] = oof_t\n",
        "    te_type[idx_te.values]  = te_t\n",
        "\n",
        "# === 3) BUILDING 모델 ===\n",
        "oof_bld = np.zeros(len(Y), dtype=float)\n",
        "te_bld  = np.zeros(len(test_X), dtype=float)\n",
        "min_rows = 200  # 표본이 너무 적은 건물은 스킵하고 상위 레벨에 맡김(원하면 조정)\n",
        "for b in X[\"building_number\"].unique():\n",
        "    idx_tr = (X[\"building_number\"] == b)\n",
        "    idx_te = (test_X[\"building_number\"] == b)\n",
        "    if idx_tr.sum() < min_rows:\n",
        "        # 표본 부족 → 상위 레벨값을 그대로 사용 (fallback)\n",
        "        oof_bld[idx_tr.values] = oof_type[idx_tr.values]  # 또는 oof_global\n",
        "        te_bld[idx_te.values]  = te_type[idx_te.values]   # 또는 te_global\n",
        "        continue\n",
        "    x_b  = X.loc[idx_tr].drop(columns=[\"building_type\"])\n",
        "    xt_b = test_X.loc[idx_te].drop(columns=[\"building_type\"])\n",
        "    oof_bb, te_bb = cv_fit_predict(make_xgb, x_b, Y.loc[idx_tr, \"power_consumption\"], xt_b)\n",
        "    oof_bld[idx_tr.values] = oof_bb\n",
        "    te_bld[idx_te.values]  = te_bb\n",
        "\n",
        "# === 가중 블렌딩 ===\n",
        "Wg, Wt, Wb = 0.2, 0.4, 0.4   # 시작 가중치 (튜닝 포인트)\n",
        "oof_blend = Wg*oof_global + Wt*oof_type + Wb*oof_bld\n",
        "te_blend  = Wg*te_global  + Wt*te_type  + Wb*te_bld\n",
        "\n",
        "# 검증 SMAPE\n",
        "print(\"Hier-Ensemble SMAPE:\", smape(Y[\"power_consumption\"].values, oof_blend))\n",
        "\n",
        "# 제출\n",
        "submit = pd.read_csv(\"/kaggle/input/dataset-ee/sample_submission.csv\")\n",
        "submit[\"answer\"] = te_blend[submit.index]\n",
        "submit.to_csv(\"hier_ensemble_xgb.csv\", index=False)\n",
        "print(\"Saved → hier_ensemble_xgb.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LGBM 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGlStb9c2DC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def smape(gt, preds):\n",
        "    gt = np.array(gt, dtype=float)\n",
        "    preds = np.array(preds, dtype=float)\n",
        "    v = 2 * np.abs(preds - gt) / (np.abs(preds) + np.abs(gt) + 1e-9)\n",
        "    return float(np.mean(v) * 100)\n",
        "\n",
        "def lgb_weighted_mse(alpha=3.0):\n",
        "    \"\"\"LightGBM custom objective: asymmetric MSE (under-prediction penalty > over-prediction).\"\"\"\n",
        "    def _obj(y_true, y_pred):\n",
        "        residual = (y_true - y_pred).astype(np.float64)\n",
        "        # under-predict (residual>0) → larger gradient/hessian\n",
        "        grad = np.where(residual > 0, -2.0*alpha*residual, -2.0*residual)\n",
        "        hess = np.where(residual > 0,  2.0*alpha,          2.0)\n",
        "        return grad, hess\n",
        "    return _obj\n",
        "\n",
        "# ---------- Load data (LOCAL PATHS) ----------\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "building_info = pd.read_csv('building_info.csv')\n",
        "\n",
        "# ---------- Rename columns ----------\n",
        "train = train.rename(columns={\n",
        "    '건물번호': 'building_number',\n",
        "    '일시': 'date_time',\n",
        "    '기온(°C)': 'temperature',\n",
        "    '강수량(mm)': 'rainfall',\n",
        "    '풍속(m/s)': 'windspeed',\n",
        "    '습도(%)': 'humidity',\n",
        "    '일조(hr)': 'sunshine',\n",
        "    '일사(MJ/m2)': 'solar_radiation',\n",
        "    '전력소비량(kWh)': 'power_consumption'\n",
        "})\n",
        "if 'num_date_time' in train.columns:\n",
        "    train.drop('num_date_time', axis=1, inplace=True)\n",
        "\n",
        "test = test.rename(columns={\n",
        "    '건물번호': 'building_number',\n",
        "    '일시': 'date_time',\n",
        "    '기온(°C)': 'temperature',\n",
        "    '강수량(mm)': 'rainfall',\n",
        "    '풍속(m/s)': 'windspeed',\n",
        "    '습도(%)': 'humidity',\n",
        "    '일조(hr)': 'sunshine',\n",
        "    '일사(MJ/m2)': 'solar_radiation',\n",
        "    '전력소비량(kWh)': 'power_consumption'\n",
        "})\n",
        "if 'num_date_time' in test.columns:\n",
        "    test.drop('num_date_time', axis=1, inplace=True)\n",
        "\n",
        "building_info = building_info.rename(columns={\n",
        "    '건물번호': 'building_number',\n",
        "    '건물유형': 'building_type',\n",
        "    '연면적(m2)': 'total_area',\n",
        "    '냉방면적(m2)': 'cooling_area',\n",
        "    '태양광용량(kW)': 'solar_power_capacity',\n",
        "    'ESS저장용량(kWh)': 'ess_capacity',\n",
        "    'PCS용량(kW)': 'pcs_capacity'\n",
        "})\n",
        "\n",
        "translation_dict = {\n",
        "    '건물기타': 'Other Buildings',\n",
        "    '공공': 'Public',\n",
        "    '학교': 'University',\n",
        "    '백화점': 'Department Store',\n",
        "    '병원': 'Hospital',\n",
        "    '상용': 'Commercial',\n",
        "    '아파트': 'Apartment',\n",
        "    '연구소': 'Research Institute',\n",
        "    'IDC(전화국)': 'IDC',\n",
        "    '호텔': 'Hotel'\n",
        "}\n",
        "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
        "\n",
        "# Feature flags for solar/ess presence\n",
        "building_info['solar_power_utility'] = np.where(building_info['solar_power_capacity'] != '-', 1, 0)\n",
        "building_info['ess_utility'] = np.where(building_info['ess_capacity'] != '-', 1, 0)\n",
        "\n",
        "# Merge static info\n",
        "train = pd.merge(train, building_info, on='building_number', how='left')\n",
        "test  = pd.merge(test,  building_info, on='building_number', how='left')\n",
        "\n",
        "# ---------- Datetime features ----------\n",
        "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
        "test['date_time']  = pd.to_datetime(test['date_time'],  format='%Y%m%d %H')\n",
        "\n",
        "for df in (train, test):\n",
        "    df['hour'] = df['date_time'].dt.hour\n",
        "    df['day'] = df['date_time'].dt.day\n",
        "    df['month'] = df['date_time'].dt.month\n",
        "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
        "\n",
        "# ---------- Per-day aggregation features ----------\n",
        "def calculate_day_values(dataframe, target_column, output_column, aggregation_func):\n",
        "    grouped = dataframe.groupby(['building_number', 'month', 'day'])[target_column].agg(aggregation_func)\n",
        "    result = {}\n",
        "    for (b, m, d), v in grouped.items():\n",
        "        result.setdefault(b, {}).setdefault(m, {})[d] = v\n",
        "    dataframe[output_column] = [\n",
        "        result.get(r['building_number'], {}).get(r['month'], {}).get(r['day'], np.nan)\n",
        "        for _, r in dataframe.iterrows()\n",
        "    ]\n",
        "\n",
        "for df in (train, test):\n",
        "    for out, func in [('day_max_temperature', 'max'),\n",
        "                      ('day_mean_temperature', 'mean'),\n",
        "                      ('day_min_temperature', 'min')]:\n",
        "        calculate_day_values(df, 'temperature', out, func)\n",
        "    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']\n",
        "\n",
        "# ---------- Outlier removal (same rules as before) ----------\n",
        "out_ind_li = []\n",
        "out_ind_li += list(train.loc[(train['building_number']==1)  & (train['power_consumption'] < 3000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==5)  &\n",
        "                             (train['date_time'].dt.strftime('%Y%m%d').astype(int).between(20240801, 20240815)) &\n",
        "                             (train['power_consumption'] < 3200)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==7)  & (train['power_consumption'] < 3000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==12) & (train['power_consumption'] < 4000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==17) &\n",
        "                             (train['date_time'].dt.strftime('%Y%m%d').astype(int).between(20240619, 20240701)) &\n",
        "                             (train['power_consumption'] < 1500)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==19) & (train['power_consumption'] > 3000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==26) & (train['power_consumption'] < 500)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==30) & (train['power_consumption'] < 9000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==41) & (train['power_consumption'] < 2500)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==42) & (train['power_consumption'] < 3000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==44) & (train['power_consumption'] < 900)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==52) & (train['power_consumption'] < 4500)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==67) & (train['power_consumption'] < 8000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==68) & (train['power_consumption'] < 1000)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==72) &\n",
        "                             (train['date_time'].dt.strftime('%Y%m%d').astype(int).between(20240601, 20240615)) &\n",
        "                             (train['power_consumption'] > 1830)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==79) & (train['power_consumption'] < 1500)].index)\n",
        "out_ind_li += list(train.loc[(train['building_number']==92) & (train['power_consumption'] < 200)].index)\n",
        "\n",
        "print(\"특이값 제거 index 개수:\", len(out_ind_li))\n",
        "train = train.drop(index=out_ind_li).reset_index(drop=True)\n",
        "\n",
        "zero_idx = train.index[train['power_consumption'] == 0].tolist()\n",
        "print(\"제로 power_consumption 제거할 행 개수:\", len(zero_idx))\n",
        "print(\"인덱스 예시:\", zero_idx[:10])\n",
        "train.drop(index=zero_idx, inplace=True)\n",
        "print(\"남은 행 개수:\", train.shape[0])\n",
        "\n",
        "# ---------- Holiday features ----------\n",
        "holi_weekday = ['2024-06-06', '2024-08-15']\n",
        "train['holiday'] = np.where(\n",
        "    (~train['building_type'].isin(['IDC', 'Department Store'])) &\n",
        "    ((train['day_of_week'] >= 5) | (train['date_time'].dt.strftime('%Y-%m-%d').isin(holi_weekday))), 1, 0)\n",
        "test['holiday'] = np.where(\n",
        "    (~test['building_type'].isin(['IDC', 'Department Store'])) &\n",
        "    ((test['day_of_week'] >= 5) | (test['date_time'].dt.strftime('%Y-%m-%d').isin(holi_weekday))), 1, 0)\n",
        "\n",
        "target_buildings = {27, 40, 59, 63}\n",
        "def mark_second_and_fourth_sunday_holiday(df):\n",
        "    is_sunday = df['day_of_week'] == 6\n",
        "    is_2nd = df['day'].between(8, 14)\n",
        "    is_4th = df['day'].between(22, 28)\n",
        "    df.loc[df['building_number'].isin(target_buildings) & is_sunday & (is_2nd | is_4th), 'holiday'] = 1\n",
        "    return df\n",
        "\n",
        "def apply_2nd_4th_monday_off_for_b32(df):\n",
        "    is_b32 = df['building_number'] == 32\n",
        "    df.loc[is_b32, 'holiday'] = 0\n",
        "    is_monday = df['day_of_week'] == 0\n",
        "    wom = ((df['day'] - 1) // 7) + 1\n",
        "    df.loc[is_b32 & is_monday & wom.isin([2, 4]), 'holiday'] = 1\n",
        "    return df\n",
        "\n",
        "def apply_sat_only_off(df):\n",
        "    is_b2 = df['building_number'] == 2\n",
        "    df.loc[is_b2, 'holiday'] = 0\n",
        "    df.loc[is_b2 & (df['day_of_week'] == 5), 'holiday'] = 1\n",
        "    return df\n",
        "\n",
        "def clear_holiday_for_specific_buildings(df):\n",
        "    df.loc[df['building_number'].isin([58, 82, 97]), 'holiday'] = 0\n",
        "    return df\n",
        "\n",
        "for f in (mark_second_and_fourth_sunday_holiday,\n",
        "          apply_2nd_4th_monday_off_for_b32,\n",
        "          apply_sat_only_off,\n",
        "          clear_holiday_for_specific_buildings):\n",
        "    train = f(train)\n",
        "    test  = f(test)\n",
        "\n",
        "# ---------- Cyclic time features ----------\n",
        "for df in (train, test):\n",
        "    df['sin_hour'] = np.sin(2*np.pi*df['hour']/23.0)\n",
        "    df['cos_hour'] = np.cos(2*np.pi*df['hour']/23.0)\n",
        "    df['sin_date'] = -np.sin(2*np.pi*(df['month']+df['day']/31)/12)\n",
        "    df['cos_date'] = -np.cos(2*np.pi*(df['month']+df['day']/31)/12)\n",
        "    df['sin_month'] = -np.sin(2*np.pi*df['month']/12.0)\n",
        "    df['cos_month'] = -np.cos(2*np.pi*df['month']/12.0)\n",
        "    df['sin_dayofweek'] = -np.sin(2*np.pi*(df['day_of_week']+1)/7.0)\n",
        "    df['cos_dayofweek'] = -np.cos(2*np.pi*(df['day_of_week']+1)/7.0)\n",
        "\n",
        "# ---------- CDH / THI / WCT ----------\n",
        "def CDH(xs):\n",
        "    cumsum = np.cumsum(xs - 26)\n",
        "    # 12-hr rolling cumulative by \"windowed\" trick\n",
        "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
        "\n",
        "def calculate_and_add_cdh(dataframe):\n",
        "    cdhs = []\n",
        "    for i in range(1, 101):\n",
        "        temp = dataframe[dataframe['building_number'] == i]['temperature'].values\n",
        "        cdhs.append(CDH(temp))\n",
        "    return np.concatenate(cdhs)\n",
        "\n",
        "train['CDH'] = calculate_and_add_cdh(train)\n",
        "test['CDH']  = calculate_and_add_cdh(test)\n",
        "\n",
        "train['THI'] = 9/5*train['temperature'] - 0.55*(1-train['humidity']/100)*(9/5*train['humidity']-26)+32\n",
        "test['THI']  = 9/5*test['temperature']  - 0.55*(1-test['humidity']/100)*(9/5*test['humidity']-26)+32\n",
        "\n",
        "train['WCT'] = 13.12 + 0.6125*train['temperature'] - 11.37*(train['windspeed']**0.16) + 0.3965*(train['windspeed']**0.16)*train['temperature']\n",
        "test['WCT']  = 13.12 + 0.6125*test['temperature']  - 11.37*(test['windspeed']**0.16)  + 0.3965*(test['windspeed']**0.16)*test['temperature']\n",
        "\n",
        "# ---------- Hourly stats features ----------\n",
        "power_mean = pd.pivot_table(train, values='power_consumption',\n",
        "                            index=['building_number','hour','day_of_week'], aggfunc=np.mean).reset_index()\n",
        "power_mean.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_mean']\n",
        "\n",
        "power_std  = pd.pivot_table(train, values='power_consumption',\n",
        "                            index=['building_number','hour','day_of_week'], aggfunc=np.std).reset_index()\n",
        "power_std.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_std']\n",
        "\n",
        "hour_mean = pd.pivot_table(train, values='power_consumption',\n",
        "                           index=['building_number','hour'], aggfunc=np.mean).reset_index()\n",
        "hour_mean.columns = ['building_number', 'hour', 'hour_mean']\n",
        "\n",
        "hour_std  = pd.pivot_table(train, values='power_consumption',\n",
        "                           index=['building_number','hour'], aggfunc=np.std).reset_index()\n",
        "hour_std.columns = ['building_number', 'hour', 'hour_std']\n",
        "\n",
        "for df in (train, test):\n",
        "    df.merge(power_mean, on=['building_number','hour','day_of_week'], how='left')\n",
        "train = train.merge(power_mean, on=['building_number','hour','day_of_week'], how='left')\n",
        "test  = test.merge(power_mean, on=['building_number','hour','day_of_week'], how='left')\n",
        "\n",
        "train = train.merge(power_std, on=['building_number','hour','day_of_week'], how='left')\n",
        "test  = test.merge(power_std, on=['building_number','hour','day_of_week'], how='left')\n",
        "\n",
        "train = train.merge(hour_mean, on=['building_number','hour'], how='left')\n",
        "test  = test.merge(hour_mean, on=['building_number','hour'], how='left')\n",
        "\n",
        "train = train.merge(hour_std, on=['building_number','hour'], how='left')\n",
        "test  = test.merge(hour_std, on=['building_number','hour'], how='left')\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "\n",
        "# ---------- Final feature sets ----------\n",
        "X = train.drop(columns=[\n",
        "    'solar_power_capacity','ess_capacity','pcs_capacity',\n",
        "    'power_consumption','rainfall','sunshine','solar_radiation',\n",
        "    'hour','day','month','day_of_week','date_time'\n",
        "], errors='ignore')\n",
        "\n",
        "Y = train[['building_type','power_consumption']].copy()\n",
        "\n",
        "test_X = test.drop(columns=[\n",
        "    'solar_power_capacity','ess_capacity','pcs_capacity',\n",
        "    'rainfall','hour','month','day_of_week','day','date_time'\n",
        "], errors='ignore')\n",
        "\n",
        "# ---------- LGBM builder ----------\n",
        "RANDOM_SEED = 42\n",
        "KFOLD_SPLITS = 7\n",
        "\n",
        "def make_lgbm():\n",
        "    return LGBMRegressor(\n",
        "        n_estimators=5000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=10,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.5,\n",
        "        min_child_samples=20,\n",
        "        reg_lambda=0.0,\n",
        "        random_state=RANDOM_SEED,\n",
        "        objective=lgb_weighted_mse(alpha=3.0),  # custom asymmetric loss\n",
        "        # device_type='gpu',  # GPU 사용 시 주석 해제 (환경에 따라)\n",
        "    )\n",
        "\n",
        "def cv_fit_predict(model_fn, X_tr, y_tr, X_te):\n",
        "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    oof = np.zeros(len(y_tr), dtype=float)\n",
        "    te_preds = []\n",
        "    Xv = X_tr.values\n",
        "    yv = y_tr.values\n",
        "    for tr_idx, va_idx in kf.split(Xv):\n",
        "        mdl = model_fn()\n",
        "        X_tr_f, X_va_f = Xv[tr_idx], Xv[va_idx]\n",
        "        y_tr_f, y_va_f = yv[tr_idx], yv[va_idx]\n",
        "        # log-transform target\n",
        "        mdl.fit(\n",
        "            X_tr_f, np.log(y_tr_f),\n",
        "            eval_set=[(X_va_f, np.log(y_va_f))],\n",
        "            eval_metric='l2',\n",
        "            verbose=False,\n",
        "            callbacks=[],\n",
        "        )\n",
        "        oof[va_idx] = np.exp(mdl.predict(X_va_f))\n",
        "        te_preds.append(np.exp(mdl.predict(X_te.values)))\n",
        "    te_mean = np.mean(te_preds, axis=0)\n",
        "    return oof, te_mean\n",
        "\n",
        "# ---------- 1) GLOBAL ----------\n",
        "X_global  = X.drop(columns=['building_type'])\n",
        "Xt_global = test_X.drop(columns=['building_type'])\n",
        "oof_global, te_global = cv_fit_predict(make_lgbm, X_global, Y['power_consumption'], Xt_global)\n",
        "\n",
        "# ---------- 2) TYPE ----------\n",
        "oof_type = np.zeros(len(Y), dtype=float)\n",
        "te_type  = np.zeros(len(test_X), dtype=float)\n",
        "for t in X['building_type'].unique():\n",
        "    idx_tr = (X['building_type'] == t)\n",
        "    idx_te = (test_X['building_type'] == t)\n",
        "    x_t  = X.loc[idx_tr].copy()\n",
        "    xt_t = test_X.loc[idx_te].copy()\n",
        "\n",
        "    # one-hot building_number inside each type block (align columns)\n",
        "    x_t  = pd.get_dummies(x_t,  columns=['building_number'], drop_first=False)\n",
        "    xt_t = pd.get_dummies(xt_t, columns=['building_number'], drop_first=False)\n",
        "    xt_t = xt_t.reindex(columns=x_t.columns, fill_value=0)\n",
        "\n",
        "    x_t = x_t.drop(columns=['building_type'])\n",
        "    xt_keep = xt_t.drop(columns=['building_type'])\n",
        "\n",
        "    oof_t, te_t = cv_fit_predict(make_lgbm, x_t, Y.loc[idx_tr, 'power_consumption'], xt_keep)\n",
        "    oof_type[idx_tr.values] = oof_t\n",
        "    te_type[idx_te.values]  = te_t\n",
        "\n",
        "# ---------- 3) BUILDING ----------\n",
        "oof_bld = np.zeros(len(Y), dtype=float)\n",
        "te_bld  = np.zeros(len(test_X), dtype=float)\n",
        "min_rows = 200  # 표본 적은 건물은 상위 레벨 예측 사용\n",
        "for b in X['building_number'].unique():\n",
        "    idx_tr = (X['building_number'] == b)\n",
        "    idx_te = (test_X['building_number'] == b)\n",
        "    if idx_tr.sum() < min_rows:\n",
        "        oof_bld[idx_tr.values] = oof_type[idx_tr.values]\n",
        "        te_bld[idx_te.values]  = te_type[idx_te.values]\n",
        "        continue\n",
        "    x_b  = X.loc[idx_tr].drop(columns=['building_type'])\n",
        "    xt_b = test_X.loc[idx_te].drop(columns=['building_type'])\n",
        "    oof_bb, te_bb = cv_fit_predict(make_lgbm, x_b, Y.loc[idx_tr, 'power_consumption'], xt_b)\n",
        "    oof_bld[idx_tr.values] = oof_bb\n",
        "    te_bld[idx_te.values]  = te_bb\n",
        "\n",
        "# ---------- Blending ----------\n",
        "Wg, Wt, Wb = 0.2, 0.4, 0.4\n",
        "oof_blend = Wg*oof_global + Wt*oof_type + Wb*oof_bld\n",
        "te_blend  = Wg*te_global  + Wt*te_type  + Wb*te_bld\n",
        "\n",
        "print(\"Hier-Ensemble SMAPE:\", smape(Y['power_consumption'].values, oof_blend))\n",
        "\n",
        "# ---------- Save submission ----------\n",
        "submit = pd.read_csv('sample_submission.csv')\n",
        "submit['answer'] = te_blend[submit.index]\n",
        "submit.to_csv('hier_ensemble_lgbm.csv', index=False)\n",
        "print(\"Saved → hier_ensemble_lgbm.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 파일 경로 설정\n",
        "sub_xgb_path = \"hier_ensemble_xgb.csv\"          # XGB 결과\n",
        "sub_lgbm_path = \"hier_ensemble_lgbm.csv\"    # LGBM 결과\n",
        "sample_path = \"sample_submission.csv\"       # 샘플 제출\n",
        "out_path = \"ensemble_avg.csv\"               # 최종 저장 경로\n",
        "\n",
        "# 데이터 로드\n",
        "sub_xgb = pd.read_csv(sub_xgb_path)\n",
        "sub_lgbm = pd.read_csv(sub_lgbm_path)\n",
        "sample = pd.read_csv(sample_path)\n",
        "\n",
        "# 평균 앙상블 (answer 컬럼만 사용)\n",
        "ensemble_answer = (sub_xgb[\"answer\"].values + sub_lgbm[\"answer\"].values) / 2\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission = sample.copy()\n",
        "submission[\"answer\"] = ensemble_answer\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(f\"Saved → {out_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7900655,
          "sourceId": 12516640,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7920542,
          "sourceId": 12843313,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
